# Функции потерь

# **Log Loss or BCE (логарифмическая потеря или бинарная кросс-энтропия)**

Log Loss (логарифмическая потеря) - это метрика, используемая для оценки производительности моделей машинного обучения, предсказывающих вероятности принадлежности объектов к классам. Она измеряет качество предсказанных вероятностей, сравнивая их с истинными метками классов.

**Формула**:

Log Loss выражается в виде отрицательного логарифма вероятности правильной классификации для каждого объекта. Меньшие значения Log Loss указывают на лучшее качество предсказаний, а большие значения - на худшее качество.

$Log Loss = -\frac{1}{N} \sum_{i=1}^{N} \left( y_i \cdot \log(p_i) + (1 - y_i) \cdot \log(1 - p_i) \right)$

Где:

- *N* - общее количество объектов
- $*y_i*$ - истинная метка класса (0 или 1) для объекта *i*
- $*p_i*$ - предсказанная вероятность принадлежности объекта *i* к классу 1

### **Плюсы**

- Широко используется в задачах бинарной классификации, особенно когда требуется оценить вероятностные прогнозы модели.
- Интуитивно понятна и легко интерпретируется.
- Хорошо дифференцируема и подходит для оптимизации градиентными методами.

### **Минусы**

- Чувствительна к дисбалансу классов, что может привести к недооценке меньшинства классов.
- Не обладает свойством устойчивости к выбросам в данных.
- Может привести к переобучению при недостаточном контроле за регуляризацией.

# Cross Entropy (кросс-энтропия)

Кросс-энтропия (Cross Entropy) - это мера расхождения между двумя вероятностными распределениями для оценки качества моделей машинного обучения в задачах классификации. Она широко используется как функция потерь в задачах обучения с учителем.

**Интерпретация**:
Кросс-энтропия измеряет, насколько вероятности, предсказанные моделью, соответствуют истинным меткам классов. Меньшие значения кросс-энтропии указывают на более точные предсказания, а большие значения - на менее точные предсказания.

**Формула**:
Кросс-энтропия вычисляется по следующей формуле:

$CE = -\frac{1}{N} \sum_{i=1}^{N} \left( y_i \cdot \log(p_i) \right)$

Где:

- N - общее количество объектов
- $y_i$ - истинная метка класса (0 или 1) для объекта *i*
- $p_i$ - предсказанная вероятность принадлежности объекта *i* к классу 1

### **Плюсы**

- Обобщение LogLoss для многоклассовой классификации.
- Хорошо подходит для задач многоклассовой классификации, где требуется оценка вероятностей принадлежности объектов к различным классам.
- Легко оптимизируется с использованием градиентных методов.

### **Минусы**

- Чувствительность к дисбалансу классов, особенно в задачах с неравномерными классами.
- Требует больше ресурсов для вычисления, чем LogLoss, особенно когда много классов.

# Symmetric Cross Entropy

Symmetric Cross Entropy (симметричная кросс-энтропия) - это функция потерь, которая была предложена для решения проблемы дисбаланса классов в задачах классификации. Она учитывает как FP, так и FN, что позволяет модели более эффективно обучаться на несбалансированных данных.

Формула Symmetric Cross Entropy:
$SCE = -\frac{1}{N} \sum_{i=1}^{N} \left( \alpha \cdot y_i \cdot \log(p_i) + (1 - \alpha) \cdot (1 - y_i) \cdot \log(1 - p_i) \right)$

Где:

- N - общее количество объектов
- $y_i$ - истинная метка класса (0 или 1) для объекта *i*
- $p_i$ - предсказанная вероятность принадлежности объекта *i* к классу 1
- $\alpha$ - коэффициент, который позволяет учитывать FP и FN в разной степени. Обычно он принимает значения от 0 до 1.

Symmetric Cross Entropy помогает модели более эффективно учиться на дисбалансированных данных, учитывая как ошибки первого, так и второго рода.

# Focal Loss

Focal Loss (фокусировочная функция потерь) - это функция потерь, предложенная для решения проблемы дисбаланса классов и сосредоточена на снижении влияния легко классифицируемых примеров (часто называемых фоновыми примерами).

Формула Focal Loss:
$FL = -\frac{1}{N} \sum_{i=1}^{N} \left( (1 - p_i)^\gamma \cdot y_i \cdot \log(p_i) + p_i^\gamma \cdot (1 - y_i) \cdot \log(1 - p_i) \right)$

Где:

- N - общее количество объектов
- $y_i$ - истинная метка класса (0 или 1) для объекта *i*
- $p_i$ - предсказанная вероятность принадлежности объекта *i* к классу 1
- $γ$ (gamma) - параметр, который управляет влиянием редких классов. Обычно он принимает значения от 0 до 5.

### **Плюсы**

- Эффективно справляется с проблемой дисбаланса классов за счет уменьшения влияния легко классифицируемых примеров.
- Позволяет управлять влиянием редких классов с помощью параметра $*γ*$.
- Хорошо подходит для задач сильно дисбалансированных классов.

### **Минусы**

- Может быть чувствительна к выбросам в данных.
- Требует дополнительной настройки гиперпараметров, таких как параметр $*γ*$.
- Может потребовать больше времени для обучения, особенно на больших наборах данных.