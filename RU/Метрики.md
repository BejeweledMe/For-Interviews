# Метрики

# Precision (точность)

Precision (точность) - это метрика, оценивающая долю истинно положительных объектов, среди всех объектов, которые мы посчитали положительными.

Формула:
$Precision = TP / (TP + FP)$

Где:

- TP (True Positives) - количество верно предсказанных положительных объектов
- FP (False Positives) - количество неверно предсказанных положительных объектов

Точность измеряется в диапазоне от 0 до 1, где 1 означает идеальную точность.

# Recall (полнота или TPR, True Positive Rate)

Recall (полнота) - это метрика, оценивающая долю положительных объектов, которые были правильно определены моделью, среди всех положительных объектов.

Формула:
$Recall = TP / (TP + FN)$

Где:

- TP (True Positives) - количество верно предсказанных положительных объектов
- FN (False Negatives) - количество положительных объектов, неверно предсказанных как отрицательные

Полнота также измеряется в диапазоне от 0 до 1, где 1 означает идеальную полноту.

# F-мера

F-мера - это гармоническое среднее между точностью и полнотой. Она представляет собой сбалансированную метрику, которая учитывает как точность, так и полноту.

Формула:
$F = 2 * (Precision * Recall) / (Precision + Recall)$

F-мера также находится в диапазоне от 0 до 1, где 1 означает идеальный баланс между точностью и полнотой.

FPR (False Positive Rate) - это доля неверно предсказанных отрицательных объектов относительно общего количества отрицательных объектов в данных.

Формула для вычисления FPR:
$FPR = \frac{FP}{FP + TN}$

где:

- FP (False Positives) - количество неверно предсказанных положительных объектов,
- TN (True Negatives) - количество верно предсказанных отрицательных объектов.

ROC-кривая используется для визуализации зависимости между TPR и FPR для различных порогов классификации и оценки производительности бинарных классификаторов.

# Accuracy (точность)

Accuracy (точность) - это метрика, оценивающая общую правильность предсказаний модели. Она измеряет долю правильно классифицированных объектов по отношению к общему количеству объектов.

Формула:
$Accuracy = (TP + TN) / (TP + TN + FP + FN)$

Где:

- TP (True Positives) - количество верно предсказанных положительных объектов
- TN (True Negatives) - количество верно предсказанных отрицательных объектов
- FP (False Positives) - количество неверно предсказанных положительных объектов
- FN (False Negatives) - количество положительных объектов, неверно предсказанных как отрицательные

Точность измеряется в диапазоне от 0 до 1, где 1 означает идеальную точность.

Accuracy хорошо подходит для сбалансированных классов, когда количество положительных и отрицательных объектов примерно одинаково. Однако для несбалансированных классов Accuracy может быть неинформативной метрикой, особенно если один класс встречается значительно чаще, чем другой.

# **FPR (False Positive Rate)**

FPR (False Positive Rate) - это метрика, оценивающая долю неверно предсказанных отрицательных объектов среди всех отрицательных объектов.

Формула:
$*FPR = FP / (FP + TN)*$

Где:

- FP (False Positives) - количество неверно предсказанных положительных объектов
- TN (True Negatives) - количество верно предсказанных отрицательных объектов

FPR также называют специфичностью модели. Она измеряется в диапазоне от 0 до 1, где 1 означает идеальную специфичность (то есть отсутствие ложноположительных предсказаний).

# ROC-кривая (Receiver Operating Characteristic curve)

ROC-кривая (Receiver Operating Characteristic curve) - это график, который показывает зависимость между чувствительностью (True Positive Rate, TPR) и специфичностью (False Positive Rate, FPR) для различных порогов классификации.

Чувствительность (TPR) выражает долю верно предсказанных положительных объектов, а FPR - долю неверно предсказанных отрицательных объектов.

ROC-кривая позволяет оценить производительность модели при различных уровнях чувствительности и специфичности, не привязываясь к конкретному порогу классификации.

ROC-кривая монотонно не убывает. Чем выше лежит кривая, тем лучше качество классификации.

Случайный классификатор (прим.1) и хороший классификатор (прим.2)

![random_classificator.png](https://github.com/BejeweledMe/For-Interviews/blob/main/RU/Метрики/random_classificator.png)

![good_classificator.png](https://github.com/BejeweledMe/For-Interviews/blob/main/RU/Метрики/good_classificator.png)

# AUC (Area Under the Curve)

AUC (Area Under the Curve) - это площадь под ROC-кривой (Receiver Operating Characteristic curve). AUC является числовой метрикой, которая измеряет общую производительность модели в сравнении с другими моделями при различных уровнях чувствительности и специфичности. Иными словами, AUC является агрегированной характеристикой качества классификации, не зависящей от соотношения цен ошибок, порогов.

Так как это площадь под кривой, то вычисляется как интеграл функции TPR(FPR) по всем порогам от 0 до 1.

## Важные характеристики

- **Интерпретация**: AUC имеет значение от 0 до 1, где 1 означает идеальную модель, которая безошибочно разделяет положительные и отрицательные классы, а 0.5 - модель, которая предсказывает случайно.
- **Как интерпретировать**:
    - AUC = 1: идеальная модель, которая безошибочно разделяет положительные и отрицательные классы.
    - AUC = 0.5: модель, которая предсказывает случайно.
    - AUC < 0.5: модель, которая хуже случайных предсказаний.
- **Сравнение моделей**: Модели с более высоким значением AUC обычно считаются более эффективными и предпочтительными.

AUC часто используется для сравнения производительности различных моделей между собой. Это особенно полезно при оценке бинарных классификаторов на несбалансированных данных. AUC не зависит от конкретного порога классификации и позволяет оценить производительность модели в целом, учитывая trade-off между чувствительностью и специфичностью.

# R2 (коэффициент детерминации)

Это метрика, оценивающая качество модели регрессии, показывающая, насколько хорошо модель соответствует данным. Он измеряет долю дисперсии зависимой переменной, которая объясняется моделью, по отношению к общей дисперсии данных.

Формула:

$SS_{res} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$

$SS_{tot} = \sum_{i=1}^{n} (y_i - \bar{y})^2$

$R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$

Где:

- $SS_{res}$ - сумма квадратов остатков
- $SS_{tot}$ - общая сумма квадратов
- $n$ - количество наблюдений
- $y_i$ - фактическое значение зависимой переменной для i-го наблюдения
- $\hat{y}_i$ - предсказанное значение зависимой переменной для i-го наблюдения
- $\bar{y}$ - среднее значение зависимой переменной.

Коэффициент детерминации принимает значения от 0 до 1, где 1 означает идеальное соответствие модели данным, а 0 означает, что модель не объясняет вариацию целевой переменной лучше, чем простое среднее.

**Преимущества:**

- Простота интерпретации: чем ближе R2 к 1, тем лучше модель объясняет данные.
- Полезен для сравнения моделей: позволяет сравнивать качество различных моделей регрессии.

**Недостатки:**

- Может быть переоценен: R2 увеличивается с добавлением новых предикторов, даже если они не улучшают модель.
- Не подходит для нелинейных моделей: R2 не является информативной метрикой для моделей, которые не являются линейными по отношению к параметрам.

R2 является важной метрикой для оценки качества моделей регрессии, но следует использовать вместе с другими метриками, особенно если модель не является линейной или существует риск переобучения.