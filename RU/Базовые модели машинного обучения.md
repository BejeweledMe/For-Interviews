# Базовые модели машинного обучения

# Линейная регрессия

Линейная регрессия - это метод, используемый для моделирования отношений между зависимой переменной (выходным значением) и одной или несколькими независимыми переменными (входными значениями). Она предполагает линейную зависимость между входными и выходными данными.

Формула для линейной регрессии:

$y = w_1x_1 + w_2x_2 + \ldots + w_nx_n + b$

Где:

- $y$ - предсказанное значение.
- $x_1, x_2, \ldots, x_n$ - значения признаков.
- $w_1, w_2, \ldots, w_n$ - веса (коэффициенты) при соответствующих признаках.
- $b$ - смещение (bias).
1. **Формула функции потерь:**
$L = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$
2. **Как обновляются веса:**
$w := w - \alpha \frac{\partial L}{\partial w}$
$b := b - \alpha \frac{\partial L}{\partial b}$

Цель линейной регрессии - найти наилучшую подходящую линию (или плоскость в многомерном случае), которая минимизирует ошибку между фактическими и предсказанными значениями. Это обычно делается путем нахождения значений весов и смещения, которые минимизируют среднеквадратичную ошибку (Mean Squared Error, MSE) или другую выбранную функцию потерь.

Линейная регрессия часто используется для прогнозирования числовых значений и анализа отношений между переменными.

# Логистическая регрессия

Логистическая регрессия - это метод классификации, который используется для оценки вероятности принадлежности наблюдения к определенному классу. В отличие от линейной регрессии, где выходное значение является непрерывной переменной, в логистической регрессии выходное значение представляет собой вероятность того, что наблюдение относится к определенному классу.

Формула для логистической регрессии:

$p(y=1 | \mathbf{x}) = \frac{1}{1 + e^{-(\mathbf{w}^T\mathbf{x} + b)}}$

Где:

- $p(y=1 | \mathbf{x})$ - вероятность того, что наблюдение $\mathbf{x}$ принадлежит классу 1.
- $\mathbf{x}$ - вектор признаков.
- $\mathbf{w}$ - вектор весов (коэффициентов) при соответствующих признаках.
- $b$ - смещение (bias).

Функция $\frac{1}{1 + e^{-z}}$, где $z = \mathbf{w}^T\mathbf{x} + b$, называется сигмоидной функцией (или логистической функцией). Она преобразует выход модели в диапазон от 0 до 1, интерпретируемый как вероятность.

1. **Формула функции потерь:**
$L = -\frac{1}{N} \sum_{i=1}^{N} (y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i))$
2. **Как обновляются веса:**
$w := w - \alpha \frac{\partial L}{\partial w}$
$b := b - \alpha \frac{\partial L}{\partial b}$

Цель логистической регрессии - найти оптимальные значения весов и смещения, которые минимизируют функцию потерь. Это позволяет модели классифицировать новые наблюдения и оценивать их вероятности принадлежности к различным классам.

# Метод k-ближайших соседей (KNN)

Метод k-ближайших соседей (KNN) - это простой и интуитивно понятный метод классификации и регрессии. В основе его работы лежит идея о том, что объекты, близкие в пространстве признаков, склонны иметь схожие значения целевой переменной.

## Классификация с помощью KNN

1. **Шаг обучения**: В процессе обучения модели KNN сохраняет все обучающие данные.
2. **Шаг предсказания**: Для нового наблюдения алгоритм находит k ближайших соседей из обучающих данных и определяет класс, к которому принадлежит большинство из этих соседей.
    - Если k=1, то новому наблюдению присваивается класс его ближайшего соседа.
    - Если k > 1, то применяется голосование по к ближайшим соседям, и класс считается наиболее часто встречающимся среди них.

## Регрессия с помощью KNN

1. **Шаг обучения**: Так же, как и для классификации, KNN сохраняет все обучающие данные.
2. **Шаг предсказания**: Для нового наблюдения алгоритм находит k ближайших соседей и использует их для вычисления среднего (для регрессии) или медианы (для робастности к выбросам) и возвращает это значение в качестве предсказания для нового наблюдения.

## Особенности KNN

- **Простота в реализации**: KNN - один из самых простых алгоритмов машинного обучения, легко понять и реализовать.
- **Неявность модели**: KNN не создает явной модели, что может быть как преимуществом, так и недостатком, в зависимости от контекста.
- **Параметр k**: Выбор значения k имеет важное значение. Маленькое значение k может привести к переобучению, тогда как слишком большое значение k может привести к упрощению модели.
- **Зависимость от метрики**: Выбор метрики расстояния также играет важную роль в работе KNN. Например, Евклидово расстояние - наиболее распространенная метрика, но она может быть неэффективной в пространствах высокой размерности.

# Деревья решений

Деревья решений - это мощный метод машинного обучения, который может использоваться как для классификации, так и для регрессии. Они представляют собой графическую модель решений, состоящую из узлов и ребер.

## Принцип работы

1. **Шаг обучения**: В процессе обучения дерево решений итеративно разделяет пространство признаков на подмножества на основе значений признаков, чтобы минимизировать неопределенность (например, энтропию для классификации или среднеквадратичную ошибку для регрессии).
2. **Шаг предсказания**: Для нового наблюдения дерево проходит через условия в каждом узле, пока не достигнет листового узла, который определяет прогнозное значение.

## Особенности деревьев решений

- **Интерпретируемость**: Деревья решений легко интерпретируются, поскольку их структура легко визуализируется и объясняется.
- **Неявность модели**: Хотя деревья решений могут быть интерпретируемы, они также могут стать сложными и переобученными при глубоком разделении.
- **Неустойчивость к изменениям в данных**: Деревья решений могут быть неустойчивыми к небольшим изменениям в данных обучения, что может привести к построению разных деревьев для одного и того же набора данных.
- **Подверженность переобучению**: При недостаточном ограничении глубины или минимального числа объектов в листе деревья решений могут сильно переобучаться.

# Random Forest (Случайный лес)

Random Forest - это алгоритм машинного обучения, который использует комбинацию нескольких деревьев решений для решения задач классификации или регрессии. Он основан на методе ансамблирования, где несколько моделей комбинируются для улучшения общего качества прогнозирования.

## Принцип работы

1. **Бутстрэп выборка**: Сначала создается несколько случайных подвыборок (бутстрэп выборок) из исходного набора данных путем выбора случайной выборки с заменой.
2. **Построение деревьев**: Для каждой бутстрэп выборки строится отдельное дерево решений. При построении каждого дерева на каждом разделении в узле выбирается подмножество случайно выбранных признаков.
3. **Голосование**: При классификации или усреднении при регрессии результаты всех деревьев объединяются для принятия окончательного решения. В случае классификации, класс, который набирает наибольшее количество голосов, становится окончательным прогнозом.

Построение отдельной модели для каждой отдельной подвыборки бутстрэпа с дальнейшим аггрегированием предсказаний называется баггингом (Bootstrap Aggregating).

## Особенности Random Forest

- **Устойчивость к переобучению**: Использование ансамбля деревьев решений помогает снизить переобучение по сравнению с отдельными деревьями.
- **Высокая точность**: За счет использования множества деревьев, Random Forest может обеспечить высокую точность прогнозирования.
- **Устойчивость к выбросам**: Random Forest хорошо справляется с выбросами в данных благодаря способности усреднять прогнозы из нескольких деревьев.
- **Способность к обработке больших наборов данных**: Поскольку каждое дерево строится независимо друг от друга, Random Forest хорошо масштабируется на большие наборы данных.

## Методы и нововведения

- **Бутстрэп выборка**: Использование бутстрэп выборки позволяет сгенерировать множество разнообразных деревьев, что улучшает обобщающую способность модели.
- **Случайный выбор признаков**: При каждом разделении в узле дерева выбирается только подмножество случайно выбранных признаков, что помогает уменьшить корреляцию между деревьями и повысить разнообразие моделей.
- **Усреднение прогнозов**: Вместо одного прогноза отдельного дерева Random Forest усредняет прогнозы всех деревьев, что уменьшает дисперсию прогноза и повышает его устойчивость.

# Градиентный бустинг

Градиентный бустинг - это ансамблевый метод машинного обучения, который строит аддитивную модель, добавляя к ней новые модели на каждом шаге, минимизируя функцию потерь. Он использует градиентный спуск для оптимизации функции потерь и построения последовательности деревьев решений.

## Основные концепции

### 1. Деревья решений в качестве базовых моделей

- Градиентный бустинг использует деревья решений как базовые модели (обычно решающие деревья), которые обучаются последовательно.

### 2. Аддитивное построение модели

- Модель строится аддитивно, добавляя новые деревья к существующей модели на каждом шаге.

### 3. Минимизация функции потерь

- На каждом шаге обучения модели минимизируется функция потерь, например, среднеквадратичная ошибка (MSE) для задач регрессии или кросс-энтропия для задач классификации.

## Процесс обучения

### 1. Инициализация

- Начинаем с некоторой простой модели, например, константного значения для задачи регрессии или распределения вероятностей для задачи классификации.

### 2. Итеративное обновление

- Для каждой итерации добавляем новое дерево решений к текущей модели.
- Новое дерево обучается на остатках предыдущей модели, то есть на разнице между фактическими значениями и предсказаниями текущей модели (если таргет 1, модель предсказала 0.7, то следующая модель будет учиться предсказывать (1 - 0.7) и тд.).

### 3. Вычисление градиента

- Вычисляем градиент функции потерь по отношению к предсказаниям текущей модели.

### 4. Обучение дерева

- Строим новое дерево, чтобы минимизировать ошибку градиента.

### 5. Добавление к модели

- Новое дерево добавляется к текущей модели с некоторым коэффициентом (learning rate), умноженным на предсказания нового дерева.

### 6. Обновление предсказаний

- Обновляем предсказания модели, добавляя предсказания нового дерева к текущим предсказаниям.

## Основные нововведения

### 1. Градиентный спуск

- Использование градиентного спуска для оптимизации функции потерь, что позволяет эффективно настраивать модель. При помощи градиентного спуска мы настраиваем коэффициенты (веса) деревьев для их аггрегации.

### 2. Регуляризация

- Применение регуляризации для предотвращения переобучения, например, через ограничение глубины деревьев или коэффициентов.

### 3. Стратегии обучения

- Использование различных стратегий обучения, таких как стохастический градиентный бустинг или усредненный градиентный бустинг, для повышения стабильности и обобщающей способности модели.

Градиентный бустинг является мощным методом машинного обучения, который широко применяется в решении различных задач, таких как регрессия и классификация, благодаря своей способности строить сложные модели и обладать высокой предсказательной способностью.